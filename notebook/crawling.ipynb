{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from lxml import html\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# For database connection\n",
        "from sqlalchemy import create_engine, text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "user-agents",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a list of user agents for random selection\n",
        "user_agents = [\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "setup-headers",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up headers with a random User-Agent\n",
        "headers = {\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
        "    \"Accept-Language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
        "    \"Cache-Control\": \"no-cache\",\n",
        "    \"Connection\": \"keep-alive\",\n",
        "    \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
        "    \"Origin\": \"https://sillok.history.go.kr\",\n",
        "    \"Pragma\": \"no-cache\",\n",
        "    \"Referer\": \"https://sillok.history.go.kr/mc/inspectionMonthList.do\",\n",
        "    \"Sec-Fetch-Dest\": \"document\",\n",
        "    \"Sec-Fetch-Mode\": \"navigate\",\n",
        "    \"Sec-Fetch-Site\": \"same-origin\",\n",
        "    \"Sec-Fetch-User\": \"?1\",\n",
        "    \"Upgrade-Insecure-Requests\": \"1\",\n",
        "    \"User-Agent\": random.choice(user_agents),\n",
        "    \"sec-ch-ua\": '\"Google Chrome\";v=\"125\", \"Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24\"',\n",
        "    \"sec-ch-ua-mobile\": \"?0\",\n",
        "    \"sec-ch-ua-platform\": '\"macOS\"',\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-fetch-month",
      "metadata": {},
      "source": [
        "### Function: fetch_month_ids\n",
        "\n",
        "This function takes a `king_id` as input and sends a POST request to the appropriate URL (depending on the type of `king_id`).\n",
        "\n",
        "It parses the HTML response to extract month IDs and month names using XPath and regular expressions. A random delay is added after the request for safety.\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "- `king_id`: A string identifier (e.g., `msilok_001` or `qsilok_001`).\n",
        "\n",
        "**Returns:**\n",
        "\n",
        "- A list of tuples, each containing a month ID and its corresponding month name.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "code-fetch-month",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_month_ids(king_id):\n",
        "    # Prepare form data for the POST request\n",
        "    data = {\"id\": king_id}\n",
        "\n",
        "    # Choose URL based on king_id type\n",
        "    url = (\n",
        "        \"https://sillok.history.go.kr/mc/inspectionMonthList.do\"\n",
        "        if king_id[0] == \"m\"\n",
        "        else \"https://sillok.history.go.kr/mc/inspectionMonthList.do?treeType=C\"\n",
        "    )\n",
        "\n",
        "    # Send POST request\n",
        "    response = requests.post(url, headers=headers, data=data)\n",
        "\n",
        "    # Safety pause to avoid overwhelming the server\n",
        "    time.sleep(random.uniform(1, 3))\n",
        "\n",
        "    # Parse HTML response\n",
        "    text = response.text\n",
        "    month_url = html.fromstring(text).xpath(\n",
        "        '//*[@id=\"cont_area\"]/div/div[2]/ul[2]/li/ul/li/a/@href'\n",
        "    )\n",
        "\n",
        "    # Extract month IDs and names using regular expressions\n",
        "    month_id = [re.search(r\"([m,q]silok_.*?)'\", month).group(1) for month in month_url]\n",
        "    month_name = [re.search(r\"(\\d{4}년 .*월?)'\", month).group(1) for month in month_url]\n",
        "\n",
        "    return list(zip(month_id, month_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-fetch-day",
      "metadata": {},
      "source": [
        "### Function: fetch_day_ids\n",
        "\n",
        "This function accepts a tuple containing a month ID and its name, and sends a POST request to fetch day IDs and associated date information.\n",
        "\n",
        "It parses the HTML response using XPath and regular expressions, and returns a list of tuples with day IDs and date info. A safety pause is included after the request.\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "- `month_id`: A tuple (`month_id`, `month_name`).\n",
        "\n",
        "**Returns:**\n",
        "\n",
        "- A list of tuples, each containing a day ID and the corresponding date information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "code-fetch-day",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_day_ids(month_id):\n",
        "    data = {\"id\": month_id[0], \"dateInfo\": month_id[1]}\n",
        "    url = (\n",
        "        \"https://sillok.history.go.kr/mc/inspectionDayList.do?treeType=M\"\n",
        "        if month_id[0][0] == \"m\"\n",
        "        else \"https://sillok.history.go.kr/mc/inspectionDayList.do?treeType=C\"\n",
        "    )\n",
        "    response = requests.post(url, headers=headers, data=data)\n",
        "\n",
        "    time.sleep(random.uniform(1, 3))\n",
        "\n",
        "    text = response.text\n",
        "    days = html.fromstring(text).xpath(\n",
        "        '//*[@id=\"cont_area\"]/div/div[1]/div/span[2]/ul/li/a/@href'\n",
        "    )\n",
        "\n",
        "    day_ids = [re.search(r\"([m,q]silok_.*?)'\", day).group(1) for day in days]\n",
        "    date_info = [re.findall(r\"'([^']*)'\", day)[-1] for day in days]\n",
        "    return list(zip(day_ids, date_info))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-fetch-article",
      "metadata": {},
      "source": [
        "### Function: fetch_article_ids\n",
        "\n",
        "This function takes a tuple with a day ID and its associated date information and sends a POST request to obtain article IDs for that day.\n",
        "\n",
        "It parses the HTML response using XPath to extract the article identifiers. A random delay is added after the request.\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "- `day_id`: A tuple (`day_id`, `date_info`).\n",
        "\n",
        "**Returns:**\n",
        "\n",
        "- A list of article IDs as strings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "code-fetch-article",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_article_ids(day_id):\n",
        "    data = {\"id\": day_id[0], \"dateInfo\": day_id[1]}\n",
        "    response = requests.post(\n",
        "        \"https://sillok.history.go.kr/mc/inspectionDayList.do\",\n",
        "        headers=headers,\n",
        "        data=data,\n",
        "    )\n",
        "\n",
        "    time.sleep(random.uniform(1, 3))\n",
        "\n",
        "    text = response.text\n",
        "    articles = html.fromstring(text).xpath(\n",
        "        \"//*[@id='cont_area']/div/div[3]/div/div[1]/ul/li/a/@id\"\n",
        "    )\n",
        "    articles = [str(a) for a in articles]\n",
        "    return articles"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-process-king",
      "metadata": {},
      "source": [
        "### Function: process_king_ids\n",
        "\n",
        "This function orchestrates the entire scraping process:\n",
        "\n",
        "1. It creates a list of king IDs (both `msilok` and `qsilok` types).\n",
        "2. Iterates over these IDs to fetch month IDs, then day IDs, and finally article IDs.\n",
        "3. Aggregates all article IDs into a single list.\n",
        "\n",
        "**Returns:**\n",
        "\n",
        "- A list of article IDs collected from the website.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6efa7be",
      "metadata": {},
      "outputs": [],
      "source": [
        "month_ids = []\n",
        "\n",
        "# Generate king IDs for both types\n",
        "mking_ids = [f\"msilok_{i:03d}\" for i in range(1, 16)]\n",
        "qking_ids = [f\"qsilok_{i:03d}\" for i in range(1, 14)]\n",
        "king_ids = mking_ids + qking_ids\n",
        "\n",
        "# Fetch month IDs for each king\n",
        "for king_id in tqdm(\n",
        "    king_ids, desc=\"Fetching month IDs from each king\", total=len(king_ids)\n",
        "):\n",
        "    try:\n",
        "        months = fetch_month_ids(king_id)\n",
        "        month_ids.extend(months)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch month IDs for {king_id}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"Total {len(month_ids)} months fetched\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efe62aa1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "\n",
        "async def fetch_day_ids_async(session, semaphore, month_id):\n",
        "    async with semaphore:\n",
        "        data = {\"id\": month_id[0], \"dateInfo\": month_id[1]}\n",
        "        url = (\n",
        "            \"https://sillok.history.go.kr/mc/inspectionDayList.do?treeType=M\"\n",
        "            if month_id[0][0] == \"m\"\n",
        "            else \"https://sillok.history.go.kr/mc/inspectionDayList.do?treeType=C\"\n",
        "        )\n",
        "\n",
        "        # Use aiohttp for async HTTP requests\n",
        "        async with session.post(\n",
        "            url, headers=headers, data=data, verify_ssl=False\n",
        "        ) as response:\n",
        "            # Safety pause with asyncio\n",
        "            await asyncio.sleep(random.uniform(1, 3))\n",
        "\n",
        "            text = await response.text()\n",
        "            tree = html.fromstring(text)\n",
        "            days = tree.xpath(\n",
        "                '//*[@id=\"cont_area\"]/div/div[1]/div/span[2]/ul/li/a/@href'\n",
        "            )\n",
        "\n",
        "            day_ids = [re.search(r\"([m,q]silok_.*?)'\", day).group(1) for day in days]\n",
        "            date_info = [re.findall(r\"'([^']*)'\", day)[-1] for day in days]\n",
        "            return list(zip(day_ids, date_info))\n",
        "\n",
        "\n",
        "# Replace the synchronous day_ids fetching with asynchronous version\n",
        "async def fetch_all_day_ids(month_ids):\n",
        "    day_ids = []\n",
        "    semaphore = asyncio.Semaphore(20)  # Limit to 20 concurrent requests\n",
        "\n",
        "    # Create ClientSession for connection pooling\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = []\n",
        "        for month in month_ids:\n",
        "            task = asyncio.create_task(fetch_day_ids_async(session, semaphore, month))\n",
        "            tasks.append(task)\n",
        "\n",
        "        # Process tasks with a progress bar\n",
        "        for task in tqdm_asyncio(\n",
        "            asyncio.as_completed(tasks),\n",
        "            desc=\"Fetching day IDs from each month\",\n",
        "            total=len(tasks),\n",
        "        ):\n",
        "            try:\n",
        "                result = await task\n",
        "                day_ids.extend(result)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to fetch day IDs: {e}\")\n",
        "                continue\n",
        "\n",
        "    return day_ids\n",
        "\n",
        "\n",
        "# Execute the async function in Jupyter notebook\n",
        "day_ids = []\n",
        "\n",
        "# This is how you run async code in Jupyter\n",
        "\n",
        "\n",
        "nest_asyncio.apply()  # This allows asyncio to work in Jupyter\n",
        "\n",
        "# Run the async function\n",
        "day_ids = asyncio.run(fetch_all_day_ids(month_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-process-king",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def fetch_article_ids(session, day_id, semaphore):\n",
        "    async with semaphore:\n",
        "        data = {\"id\": day_id[0], \"dateInfo\": day_id[1]}\n",
        "        async with session.post(\n",
        "            \"https://sillok.history.go.kr/mc/inspectionDayList.do\",\n",
        "            headers=headers,\n",
        "            data=data,\n",
        "            verify_ssl=False,\n",
        "        ) as response:\n",
        "            text = await response.text()\n",
        "        # Mimic random sleep between 1 and 3 seconds\n",
        "        await asyncio.sleep(random.uniform(0, 1))\n",
        "        tree = html.fromstring(text)\n",
        "        articles = tree.xpath(\"//*[@id='cont_area']/div/div[3]/div/div[1]/ul/li/a/@id\")\n",
        "        return [str(a) for a in articles]\n",
        "\n",
        "\n",
        "async def main(day_ids):\n",
        "    article_ids = []\n",
        "    semaphore = asyncio.Semaphore(30)  # Limit to 20 concurrent requests\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [fetch_article_ids(session, day, semaphore) for day in day_ids]\n",
        "        for task in tqdm(\n",
        "            asyncio.as_completed(tasks),\n",
        "            total=len(tasks),\n",
        "            desc=\"Fetching article IDs from each day\",\n",
        "        ):\n",
        "            try:\n",
        "                articles = await task\n",
        "                article_ids.extend(articles)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to fetch article IDs: {e}\")\n",
        "    return article_ids\n",
        "\n",
        "\n",
        "nest_asyncio.apply()\n",
        "# In a Jupyter Notebook cell, you can run:\n",
        "article_ids = asyncio.run(main(day_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-execute-scrape",
      "metadata": {},
      "source": [
        "### Execute the Scraping Process\n",
        "\n",
        "Call the `process_king_ids` function to start the scraping process. The returned list of article IDs will be used for downloading individual article pages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "execute-scrape",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute the scraping process and collect article IDs\n",
        "article_ids = process_king_ids()\n",
        "print(f\"Total {len(article_ids)} article IDs fetched\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9422d258",
      "metadata": {},
      "outputs": [],
      "source": [
        "day_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-download-html",
      "metadata": {},
      "source": [
        "### Download HTML Pages\n",
        "\n",
        "This block downloads the HTML content of each article page and saves it to the `data/mqsillok/raw/html` directory. A safety pause is added between requests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "download-html",
      "metadata": {},
      "outputs": [],
      "source": [
        "save_dir = \"data/mqsillok/raw/html\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "for article_id in tqdm(\n",
        "    article_ids, desc=\"Downloading HTML pages\", total=len(article_ids)\n",
        "):\n",
        "    try:\n",
        "        url = f\"https://sillok.history.go.kr/mc/id/{article_id}\"\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        # Safety pause\n",
        "        time.sleep(random.uniform(1, 3))\n",
        "        html_content = response.text\n",
        "\n",
        "        # Save HTML content to a file\n",
        "        with open(\n",
        "            os.path.join(save_dir, f\"{article_id}.html\"), \"w\", encoding=\"utf-8\"\n",
        "        ) as f:\n",
        "            f.write(html_content)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download HTML for {article_id}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-extract-data",
      "metadata": {},
      "source": [
        "### Extract Data from HTML Pages\n",
        "\n",
        "This block reads the saved HTML files, extracts information (title, year, date, content, and king) from each page using XPath, and compiles the data into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "extract-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = []\n",
        "for article_id in tqdm(\n",
        "    article_ids, desc=\"Extracting data from HTML pages\", total=len(article_ids)\n",
        "):\n",
        "    try:\n",
        "        file_path = os.path.join(save_dir, f\"{article_id}.html\")\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            html_content = f.read()\n",
        "\n",
        "        page = html.fromstring(html_content)\n",
        "\n",
        "        # Extract title\n",
        "        title = page.xpath(\n",
        "            \"/html/body/div[2]/div[2]/div[2]/div/div[1]/div/span/text()\"\n",
        "        )[0].strip()\n",
        "\n",
        "        # Extract year (remove the '년' character)\n",
        "        year_text = page.xpath(\n",
        "            \"/html/body/div[2]/div[2]/div[2]/div/div[1]/div/span/span/text()\"\n",
        "        )[0].strip()\n",
        "        year = int(year_text.replace(\"년\", \"\"))\n",
        "\n",
        "        # Extract content text\n",
        "        content = \" \".join(\n",
        "            p.text_content().strip()\n",
        "            for p in page.xpath(\"/html/body/div[2]/div[2]/div[2]/div/div[3]/div/div//p\")\n",
        "        )\n",
        "\n",
        "        # Extract date\n",
        "        date = page.xpath(\"/html/body/div[2]/div[2]/div[2]/div/ul[1]/li[6]/a/text()\")[\n",
        "            0\n",
        "        ].strip()\n",
        "\n",
        "        # Extract king information from the URL and clean it up\n",
        "        king_raw = page.xpath(\n",
        "            \"/html/body/div[2]/div[2]/div[2]/div/ul[1]/li[3]/a/@href\"\n",
        "        )[0].strip()\n",
        "        king = king_raw.split(\", \")[3].strip(\";\").strip(\"')\").replace(\"實錄\", \"\")\n",
        "\n",
        "        info = {\n",
        "            \"title\": title,\n",
        "            \"year\": year,\n",
        "            \"date\": date,\n",
        "            \"content\": content,\n",
        "            \"king\": king,\n",
        "        }\n",
        "        data.append(info)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to extract data for {article_id}: {e}\")\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(f\"Extracted data for {len(df)} articles\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd22d0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-save-db",
      "metadata": {},
      "source": [
        "### Save Data to PostgreSQL Database\n",
        "\n",
        "This block creates a PostgreSQL table (dropping it if it exists) and inserts the data from the DataFrame into the table using SQLAlchemy. Make sure to update the connection string with your credentials.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "save-to-db",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the PostgreSQL connection string (update with your credentials)\n",
        "db_connection_str = \"postgresql://dhg503:dhg503dhg503@localhost:54503/dhg503\"\n",
        "engine = create_engine(db_connection_str)\n",
        "\n",
        "# SQL to drop the table if it exists and create a new table\n",
        "create_table_sql = \"\"\"\n",
        "DROP TABLE IF EXISTS mqshillu;\n",
        "CREATE TABLE mqshillu (\n",
        "    title TEXT,\n",
        "    year INTEGER,\n",
        "    date TEXT,\n",
        "    content TEXT,\n",
        "    king TEXT\n",
        ");\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54e28b93",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute SQL statements within a transaction\n",
        "with engine.connect() as conn:\n",
        "    conn.execute(text(\"BEGIN\"))\n",
        "    conn.execute(text(create_table_sql))\n",
        "\n",
        "    # Insert data row by row from the DataFrame\n",
        "    for _, row in df.iterrows():\n",
        "        insert_sql = text(\n",
        "            \"\"\"\n",
        "            INSERT INTO mqshillu (title, year, date, content, king)\n",
        "            VALUES (:title, :year, :date, :content, :king)\n",
        "            \"\"\"\n",
        "        )\n",
        "        conn.execute(\n",
        "            insert_sql,\n",
        "            {\n",
        "                \"title\": row[\"title\"],\n",
        "                \"year\": row[\"year\"],\n",
        "                \"date\": row[\"date\"],\n",
        "                \"content\": row[\"content\"],\n",
        "                \"king\": row[\"king\"],\n",
        "            },\n",
        "        )\n",
        "\n",
        "    conn.execute(text(\"COMMIT\"))\n",
        "\n",
        "print(\"Data successfully saved to the PostgreSQL database.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "499d5c88",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
